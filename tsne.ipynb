{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import models\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from imbalance_cifar import IMBALANCECIFAR10, IMBALANCECIFAR100\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import umap\n",
    "from ipywidgets import interact\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_layer = 'last' # last or before_last\n",
    "model_checkpoint = 'checkpoint/cifar10_resnet32_CE_None_exp_0.01_seed_None_0/ckpt.best.pth.tar'\n",
    "# model_checkpoint = 'checkpoint/cifar10_resnet32_LDAM_DRW_exp_0.01_sam_0.8_sched_none_seed_None_0/ckpt.best.pth.tar'\n",
    "\n",
    "arch = 'resnet32'\n",
    "dataset = 'cifar10'\n",
    "loss_type = 'CE'\n",
    "gpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet_s(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): LambdaLayer()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): LambdaLayer()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_name  = model_checkpoint.split('/')[-2]\n",
    "\n",
    "num_classes = 100 if dataset == 'cifar100' else 10\n",
    "use_norm = True if loss_type == 'LDAM' else False\n",
    "model = models.__dict__[arch](num_classes=num_classes, use_norm=use_norm)\n",
    "\n",
    "if gpu is not None:\n",
    "        torch.cuda.set_device(gpu)\n",
    "        model = model.cuda(gpu)\n",
    "else:\n",
    "    # DataParallel will divide and allocate batch_size to all available GPUs\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "checkpoint = torch.load(model_checkpoint)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Length of val dataset:  10000\n",
      "Files already downloaded and verified\n",
      "Length of train dataset:  12406\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "val_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_val)\n",
    "val_len=len(val_dataset)\n",
    "print('Length of val dataset: ', val_len)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False,num_workers=12, pin_memory=True)\n",
    "\n",
    "imb_type = 'exp'\n",
    "imb_factor = 0.01\n",
    "rand_number = 0\n",
    "\n",
    "train_dataset = IMBALANCECIFAR10(root='./data', imb_type=imb_type, imb_factor=imb_factor, rand_number=rand_number, train=True, download=True, transform=transform_train)\n",
    "train_len=len(train_dataset)\n",
    "print('Length of train dataset: ', train_len)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=False,num_workers=12, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get intermediate layer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {'train': [], 'val': []}\n",
    "labels = {'train': [], 'val': []}\n",
    "loaders = {'train': train_loader, 'val': val_loader}\n",
    "datasets = ['train', 'val']\n",
    "\n",
    "def get_features(dataset='val'):\n",
    "    def hook(model, input, output):\n",
    "        features[dataset].append(F.avg_pool2d(output, output.size()[3]).view(output.size(0), -1).detach().cpu().numpy())\n",
    "    return hook\n",
    "\n",
    "model.eval()\n",
    "for dataset in datasets:\n",
    "    model = models.__dict__[arch](num_classes=num_classes, use_norm=use_norm)\n",
    "    if gpu is not None:\n",
    "        torch.cuda.set_device(gpu)\n",
    "        model = model.cuda(gpu)\n",
    "    else:\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "    checkpoint = torch.load(model_checkpoint)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    if embed_layer == 'before_last':\n",
    "        model.module.layer3.register_forward_hook(get_features(dataset=dataset))\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(loaders[dataset]):\n",
    "            labels[dataset].extend(target.cpu().numpy())\n",
    "            if gpu is not None:\n",
    "                input = input.cuda(gpu, non_blocking=True)\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            if embed_layer == 'last':\n",
    "                features[dataset].append(output.detach().cpu().numpy())\n",
    "\n",
    "for dataset in datasets:\n",
    "    features[dataset] = np.concatenate(features[dataset], axis=0)\n",
    "    labels[dataset] = np.array(labels[dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22406, 10)\n",
      "(22406,)\n"
     ]
    }
   ],
   "source": [
    "# high_dim_features = np.random.rand(100, 1000)\n",
    "high_dim_features = np.concatenate([features['train'], features['val']], axis=0)\n",
    "high_dim_classes = np.concatenate([labels['train'], labels['val']], axis=0)\n",
    "print(high_dim_features.shape)\n",
    "print(high_dim_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "umap shape: (22406, 2)\n",
      "tsne shape: (22406, 2)\n"
     ]
    }
   ],
   "source": [
    "# get UMAP embeddings\n",
    "reducer = umap.UMAP()\n",
    "umap_results = reducer.fit_transform(high_dim_features)\n",
    "print('umap shape:',umap_results.shape)\n",
    "tsne_results = TSNE(n_components=2).fit_transform(high_dim_features,high_dim_classes)\n",
    "print('tsne shape:',tsne_results.shape)\n",
    "df = pd.DataFrame(columns=['tsne1', 'tsne2', 'class', 'dataset'])\n",
    "df['umap1'] = umap_results[:,0]\n",
    "df['umap2'] = umap_results[:,1]\n",
    "df['tsne1'] = tsne_results[:,0]\n",
    "df['tsne2'] = tsne_results[:,1]\n",
    "df['class'] = high_dim_classes\n",
    "df['dataset'] = ['train']*train_len + ['val']*val_len\n",
    "df.to_csv(f'embeddings/{model_name}_{embed_layer}.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots\n",
    "## Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_layer = 'before_last' # last or before_last\n",
    "# model_checkpoint = 'checkpoint/cifar10_resnet32_CE_None_exp_0.01_seed_None_0/ckpt.best.pth.tar'\n",
    "model_checkpoint = 'checkpoint/cifar10_resnet32_LDAM_DRW_exp_0.01_sam_0.8_sched_none_seed_None_0/ckpt.best.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065d6c3e4f3a4dfe8f185141a2677417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dataset', options=('train', 'val'), value='train'), IntSlider(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_tsne(dataset, n_samples, embeddings='umap')>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name  = model_checkpoint.split('/')[-2]\n",
    "\n",
    "# load csv\n",
    "df = pd.read_csv(f'embeddings/{model_name}_{embed_layer}.csv')\n",
    "# scatterplot with seaborn with hue for class style for dataset. Add widgets to select dataset and number of samples\n",
    "def plot_tsne(dataset, n_samples, embeddings='umap'):\n",
    "    if embeddings == 'umap':\n",
    "        x = 'umap1'\n",
    "        y = 'umap2'\n",
    "    elif embeddings == 'tsne':\n",
    "        x = 'tsne1'\n",
    "        y = 'tsne2'\n",
    "    sns.scatterplot(data = df[df['dataset'] == dataset].sample(n=n_samples), x=x, y=y, hue='class', style='dataset', palette=sns.color_palette(\"bright\", 10), s=10)\n",
    "    sil_score = silhouette_score(df[df['dataset'] == dataset][[x, y]], df[df['dataset'] == dataset]['class'])\n",
    "    print(f'Silhouette score for {dataset} dataset: {sil_score}')\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_tsne, dataset=['train', 'val'], n_samples=(100, 10000, 1000), embeddings=['umap', 'tsne'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78117cc5f6ec4d6e85de09197320123a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Number:', options=('0', '1', '2', '3', '4', '5', '6', '7', '8', '9…"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot scatterpot with selected class colored\n",
    "\n",
    "dd = widgets.Dropdown(\n",
    "    options=[str(i) for i in range(10)],\n",
    "    value='0',\n",
    "    description='Number:')\n",
    "\n",
    "def draw_plot(num, dataset='train', embeddings='umap'):\n",
    "    if embeddings == 'umap':\n",
    "        x = 'umap1'\n",
    "        y = 'umap2'\n",
    "    elif embeddings == 'tsne':\n",
    "        x = 'tsne1'\n",
    "        y = 'tsne2'\n",
    "    color = lambda label: 'grey' if label != int(num) else 'red'\n",
    "    sns.scatterplot(data = df[df['dataset'] == dataset], x=x, y=y, hue='class', style='dataset', palette=[color(i) for i in range(10)], s=10)\n",
    "    plt.show()\n",
    "\n",
    "widgets.interactive(draw_plot, num=dd, dataset=['train', 'val'], embeddings=['umap', 'tsne'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
